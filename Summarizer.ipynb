{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automatic Text Summarization with Python\n",
    "### In today's age of information overload, it can be difficult to keep up with the amount of content available online. Automatic text summarization is a technique that can help us quickly extract the most important information from a piece of text.\n",
    "\n",
    "### In this Jupyter notebook, we'll walk through an implementation of an extractive text summarization algorithm using Python. We'll be using the requests, re, beautifulsoup4 and nltk libraries to fetch and process the content from a webpage, create a frequency table of stemmed words, score each sentence based on the frequency of its non-stop words, and generate a summary based on the top-scoring sentences.\n",
    "\n",
    "### Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### > Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import re\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### > Step 1: Fetch the Article Content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_article_content(url):\n",
    "    \"\"\"\n",
    "    Fetches the content of the webpage at the given URL and returns it as a string.\n",
    "    \"\"\"\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    article_content = ''\n",
    "    for paragraph in soup.find_all('p'):\n",
    "        article_content += paragraph.text.strip() + ' '\n",
    "    article_content = re.sub(r'\\[[0-9]*\\]', '', article_content)\n",
    "    article_content = re.sub(r'\\s+', ' ', article_content)\n",
    "    return article_content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### > Step 2: Create a Frequency Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_frequency_table(text_string):\n",
    "    \"\"\"\n",
    "    Creates a frequency table of stemmed words from the input string, excluding stopwords.\n",
    "    \"\"\"\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    words = word_tokenize(text_string)\n",
    "    stemmer = PorterStemmer()\n",
    "\n",
    "    freq_table = dict()\n",
    "    for word in words:\n",
    "        word = stemmer.stem(word)\n",
    "        if word in stop_words:\n",
    "            continue\n",
    "        if word in freq_table:\n",
    "            freq_table[word] += 1\n",
    "        else:\n",
    "            freq_table[word] = 1\n",
    "\n",
    "    return freq_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### > Step 3: Score Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_sentences(sentences, freq_table):\n",
    "    \"\"\"\n",
    "    Scores each sentence based on the frequency of its non-stop words.\n",
    "    \"\"\"\n",
    "    sentence_value = dict()\n",
    "    for sentence in sentences:\n",
    "        word_count_in_sentence = len(word_tokenize(sentence))\n",
    "        word_count_in_sentence_except_stop_words = 0\n",
    "        for word_value in freq_table:\n",
    "            if word_value in sentence.lower():\n",
    "                word_count_in_sentence_except_stop_words += 1\n",
    "                if sentence[:10] in sentence_value:\n",
    "                    sentence_value[sentence[:10]] += freq_table[word_value]\n",
    "                else:\n",
    "                    sentence_value[sentence[:10]] = freq_table[word_value]\n",
    "\n",
    "        if sentence[:10] in sentence_value:\n",
    "            sentence_value[sentence[:10]] = sentence_value[sentence[:10]] / word_count_in_sentence_except_stop_words\n",
    "\n",
    "    return sentence_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### > Step 4: Find the Average Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_average_score(sentence_value):\n",
    "    \"\"\"\n",
    "    Calculates the average score of the sentences.\n",
    "    \"\"\"\n",
    "    sum_values = sum(sentence_value.values())\n",
    "    average = sum_values / len(sentence_value)\n",
    "    return average"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### > Step 5: Generate the Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_summary(sentences, sentence_value, threshold):\n",
    "    \"\"\"\n",
    "    Generates the summary of the article using the sentence scores and threshold.\n",
    "    \"\"\"\n",
    "    summary = ''\n",
    "    for sentence in sentences:\n",
    "        if sentence[:10] in sentence_value and sentence_value[sentence[:10]] >= threshold:\n",
    "            summary += \" \" + sentence\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### > Step 6: Summarization Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_summarization(article_content):\n",
    "    \"\"\"\n",
    "    Runs the summarization algorithm on the input article content.\n",
    "    \"\"\"\n",
    "    # Create frequency table\n",
    "    freq_table = create_frequency_table(article_content)\n",
    "\n",
    "    # Tokenize sentences\n",
    "    sentences = sent_tokenize(article_content)\n",
    "\n",
    "    # Score the sentences\n",
    "    sentence_value = score_sentences(sentences, freq_table)\n",
    "\n",
    "    # Find the threshold\n",
    "    threshold = 1.3 * find_average_score(sentence_value)\n",
    "\n",
    "    # Generate the summary\n",
    "    summary = generate_summary(sentences, sentence_value, threshold)\n",
    "\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### > Step 7: Executing the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Machine Learning:  A subset of machine learning is closely related to computational statistics, which focuses on making predictions using computers, but not all machine learning is statistical learning. The study of mathematical optimization delivers methods, theory and application domains to the field of machine learning. In its application across business problems, machine learning is also referred to as predictive analytics. As a scientific endeavor, machine learning grew out of the quest for artificial intelligence (AI). In the early days of AI as an academic discipline, some researchers were interested in having machines learn from data. : 25 Machine learning (ML), reorganized and recognized as its own field, started to flourish in the 1990s. Some statisticians have adopted methods from machine learning, leading to a combined field that they call statistical learning. Instead, probabilistic bounds on the performance are quite common. If the hypothesis is less complex than the function, then the model has under fitted the data. If the complexity of the model is increased in response, then the training error decreases. In computational learning theory, a computation is considered feasible if it can be done in polynomial time. The data is known as training data, and consists of a set of training examples. Each training example has one or more inputs and the desired output, also known as a supervisory signal. Through iterative optimization of an objective function, supervised learning algorithms learn a function that can be used to predict the output associated with new inputs. Types of supervised-learning algorithms include active learning, classification and regression. The algorithms, therefore, learn from test data that has not been labeled, classified or categorized. A central application of unsupervised learning is in the field of density estimation in statistics, such as finding the probability density function. In machine learning, the environment is typically represented as a Markov decision process (MDP). As of 2022, deep learning is the dominant approach for much ongoing work in the field of machine learning. It is learning with no external rewards and no external teacher advice. In supervised feature learning, features are learned using labeled input data. In unsupervised feature learning, features are learned with unlabeled input data. Sparse coding algorithms attempt to do so under the constraint that the learned representation is sparse, meaning that the mathematical model has many zeros. In data mining, anomaly detection, also known as outlier detection, is the identification of rare items, events or observations which raise suspicions by differing significantly from the majority of the data. Typically, the anomalous items represent an issue such as bank fraud, a structural defect, medical problems or errors in a text. Robot learning is inspired by a multitude of machine learning methods, starting from supervised learning, reinforcement learning, and finally meta-learning (e.g. MAML). Rule-based machine learning is a general term for any machine learning method that identifies, learns, or evolves \"rules\" to store, manipulate or apply knowledge. Rule-based machine learning approaches include learning classifier systems, association rule learning, and artificial immune systems. Performing machine learning involves creating a model, which is trained on some training data and then can process additional data to make predictions. Typically, artificial neurons are aggregated into layers. It is one of the predictive modeling approaches used in statistics, data mining, and machine learning. In data mining, a decision tree describes data, but the resulting classification tree can be an input for decision-making. In machine learning, genetic algorithms were used in the 1980s and 1990s. Conversely, machine learning techniques have been used to improve the performance of genetic and evolutionary algorithms. Typically, machine learning models require a high quantity of reliable data in order for the models to perform accurate predictions. When training a machine learning model, machine learning engineers need to target and collect a large and representative sample of data. Overfitting is something to watch out for when training a machine learning model. In 2020, machine learning technology was used to help make diagnoses and aid researchers in developing a cure for COVID-19. Although machine learning has been transformative in some fields, machine-learning programs often fail to deliver expected results. When trained on human-made data, machine learning is likely to pick up the constitutional and unconscious biases already present in society. Because of such challenges, the effective use of machine learning may take longer to be adopted in other domains. Settling on a bad, overly complex theory gerrymandered to fit all the past training data is known as overfitting. However, these rates are ratios that fail to reveal their numerators and denominators. Because human languages contain biases, machines trained on language corpora will necessarily also learn these biases. Embedded Machine Learning is a sub-field of machine learning, where the machine learning model is run on embedded systems with limited computing resources such as wearable computers, edge devices and microcontrollers. Embedded Machine Learning could be applied through several techniques including hardware acceleration, using approximate computing, optimization of machine learning models and many more.\n"
     ]
    }
   ],
   "source": [
    "# \n",
    "if __name__ == '__main__':\n",
    "    # Define the web page URL and fetch the content\n",
    "    url = 'https://en.wikipedia.org/wiki/Machine_Learning'\n",
    "    article_content = fetch_article_content(url)\n",
    "    clean_string = url.split('/')[-1].replace('_', ' ')\n",
    "    # Generate the summary\n",
    "    summary = clean_string+\": \"+ run_summarization(article_content)\n",
    "\n",
    "    # Print the summary\n",
    "    print(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bonus: If you found the above code interesting, here is something for you.\n",
    "This is a Python code that generates a summary of a given input text using BART (Bidirectional and Auto-Regressive Transformer) transformer. BART is a state-of-the-art pre-trained language model developed by Facebook AI that can perform a wide range of natural language processing tasks. The code uses the BART model to generate a summary by first tokenizing the input text and then generating a summary using beam search. The resulting summary is decoded from the tokenized summary IDs and returned as a string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import BartTokenizer, TFBartForConditionalGeneration\n",
    "\n",
    "# def BART_generate_summary(text: str, model_name: str = \"facebook/bart-large-cnn\") -> str:\n",
    "#     \"\"\"\n",
    "#     Generate summary using BART transformer\n",
    "#     \"\"\"\n",
    "#     # Initialize tokenizer and model\n",
    "#     tokenizer = BartTokenizer.from_pretrained(model_name, from_tf=True)\n",
    "#     model = TFBartForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "#     # Tokenize input text\n",
    "#     inputs = tokenizer.batch_encode_plus([text], max_length=1024, return_tensors='tf', truncation=True)\n",
    "\n",
    "#     # Generate summary\n",
    "#     summary_ids = model.generate(inputs['input_ids'], num_beams=4, max_length=200, early_stopping=True)\n",
    "#     summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "\n",
    "#     return summary\n",
    "\n",
    "# bart_summary = BART_generate_summary(article_content)\n",
    "# print(bart_summary)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
