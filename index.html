<!DOCTYPE html>
<html lang="en">

<head>
    <style>
        section {
            /* Set the container class as the default style for all sections */
            padding-left: 10%;
            padding-right: 10%;
        }

        * {
            text-align: justify;
        }
    </style>

    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link href="https://unpkg.com/tailwindcss@^1.0/dist/tailwind.min.css" rel="stylesheet">
    <title>NLP Term Project</title>
</head>

<body>
    <header class="text-gray-600 body-font">
        <div class="container mx-auto flex flex-wrap p-5 flex-col md:flex-row items-center">
            <a class="flex title-font font-medium items-center text-gray-900 mb-4 md:mb-0">
                <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="blue" stroke-width="2"
                    stroke-linecap="round" stroke-linejoin="round" class="feather feather-type">
                    <circle cx="12" cy="12" r="10"></circle>
                    <polygon
                        points="16 6 12 6 12 2 10 2 10 6 6 6 6 8 10 8 10 12 6 12 6 14 10 14 10 18 12 18 12 14 16 14 16 12 20 12 20 8 16 8 16 6">
                    </polygon>
                </svg>
                <span class="ml-3 text-xl">Article Summarizer</span>
            </a>
            <nav
                class="md:mr-auto md:ml-4 md:py-1 md:pl-4 md:border-l md:border-gray-400	flex flex-wrap items-center text-base justify-center">
                <a herf="/" class="mr-5 hover:text-gray-900"><a href="nltk.html "> spaCy </a></a>
            </nav>
            <button
                class="inline-flex items-center bg-gray-100 border-0 py-1 px-3 focus:outline-none hover:bg-gray-200 rounded text-base mt-4 md:mt-0"><a
                    href="https://github.com/sarthakchauhan0/sarthakchauhan0.github.io">View On GitHub</a>
                <svg fill="none" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2"
                    class="w-4 h-4 ml-1" viewBox="0 0 24 24">
                    <path d="M5 12h14M12 5l7 7-7 7"></path>
                </svg>
            </button>
        </div>
    </header>

</body>
<section class="text-gray-600 body-font">
    <div class="container mx-auto flex px-5 py-24 items-center justify-center flex-col">
        <img class="lg:w-2/6 md:w-3/6 w-5/6 mb-10 object-cover object-center rounded" alt="hero"
            src="https://images.unsplash.com/photo-1542744173-05336fcc7ad4?ixlib=rb-4.0.3&ixid=MnwxMjA3fDB8MHxzZWFyY2h8Mnx8RGF0YSUyMFNjaWVuY2UlMjA3MDB4NjAwfGVufDB8fDB8fA%3D%3D&auto=format&fit=crop&w=600&q=60">
        <div class="text-center lg:w-2/3 w-full">
            <h1 class="title-font sm:text-4xl text-3xl mb-4 font-small text-gray-900">How to use the Natural Language
                ToolKit (NLTK) for text summarization?</h1>
            <p class="mb-8 leading-relaxed text-align=justify" style="margin-left: 20px; font-size: 20px;"> Text
                summarization is
                a critical component of
                Natural Language Processing (NLP), which involves condensing lengthy documents into shorter versions
                while retaining the essential information. There are two primary methods for text summarization:
                NLP-based techniques and deep learning-based techniques. In this article, we will focus on a simple
                NLP-based approach to summarize Wikipedia articles. Unlike deep learning techniques, which require a
                large amount of data and computational resources, this method relies on the Natural Language Toolkit
                (NLTK) library in Python. By the end of this article, readers will have a basic understanding of text
                summarization using NLP techniques and the NLTK library. This knowledge can be applied to various
                fields, such as research, journalism, and content creation, where summarizing large amounts of text is
                crucial for decision-making.
            </p>
            <div class="flex justify-center">
                <button
                    class="inline-flex text-white bg-green-500 border-0 py-2 px-6 focus:outline-none hover:bg-green-600 rounded text-lg"><a
                        href="https://github.com/sarthakchauhan0/sarthakchauhan0.github.io/blob/main/Summarizer.ipynb">Code</a></button>
            </div>
        </div>
    </div>
</section>

<section>


    <p class="mb-8 leading-relaxed text-align=justify" style="margin-left: 20px; font-size: 20px;"><b>INTRODUCTION</b>
    </p>
    <p class="mb-0 leading-relaxed text-align=justify" style="margin-left: 20px;">Natural Language Processing (NLP) has
        become an
        increasingly popular field, with applications in various industries such as healthcare, finance, and
        entertainment. One of the essential tasks in NLP is text summarization, which involves condensing lengthy
        articles or documents into shorter versions without losing their key information. In this tutorial, we will
        focus on a specific use case of text summarization - summarizing Wikipedia articles using the Natural
        Language
        Toolkit (NLTK). This tutorial is designed for beginners who are interested in NLP and want to learn how to
        use
        NLTK for text summarization. We will cover the basics of text preprocessing, tokenization, and summarization
        using NLTK, providing step-by-step instructions and code examples to make it easy for learners to follow
        along.
        By the end of this tutorial, learners will have a clear understanding of how to use NLTK for text
        summarization
        and gain valuable skills in NLP and practical applications of the tool.<br> <br></p>

</section>


<section>
    <div class="mb-8 leading-relaxed text-align=justify" style="margin-left: 20px;"></div>
    <p class="mb-8 leading-relaxed text-align=justify" style="margin-left: 20px; font-size: 20px;"><b>APPROACH</b></p>
    <p class="mb-0 leading-relaxed text-align=justify" style="margin-left: 20px;">Firstly, we will load the spaCy model,
        <a href="https://www.kaggle.com/datasets/snehaanbhawal/resume-dataset"
            style="color:blue; text-decoration:underline;">Resume Dataset</a>, and <a
            href="https://github.com/kingabzpro/jobzilla_ai/blob/main/jz_skill_patterns.jsonl"
            style="color:blue; text-decoration:underline;">Jobzilla skills</a> dataset directly into the entity ruler.
        We will then randomized Job categories so that 200 samples contain various job categories instead of one. <br>
        In this tutorial, we will limit our number of samples to 200 as processing 2400+ takes time.<br>
    </p>

    <pre><code class="python">
         df = pd.read_csv("Resume.csv")
         df = df.reindex(np.random.permutation(df.index))
         data = df.copy().iloc[0:200,]         
        </code></pre>

    <p class="mb-8 leading-relaxed text-align=justify" style="margin-left: 20px;"><b> Step 1: Fetch the Article Content
        </b></p>
    <p class="mb-0 leading-relaxed text-align=justify" style="margin-left: 20px;">The first step is to fetch the content
        of the
        article we want to summarize. We will use the Python requests module to send an HTTP request to the URL of the
        article we want to summarize. Then, we will use BeautifulSoup to extract the text content from the HTML page.
        Finally, we will clean the text data by removing any unwanted characters or special symbols using regular
        expressions.

    </p>

    <pre><code class="python">
        import requests
        import re
        from bs4 import BeautifulSoup
        
        def fetch_article_content(url):
            response = requests.get(url)
            soup = BeautifulSoup(response.content, 'html.parser')
            article_content = ''
            for paragraph in soup.find_all('p'):
                article_content += paragraph.text.strip() + ' '
            article_content = re.sub(r'\[[0-9]*\]', '', article_content)
            article_content = re.sub(r'\s+', ' ', article_content)
            return article_content
      </code></pre>

    <p class="mb-8 leading-relaxed text-align=justify" style="margin-left: 20px;"><b>Step 2: Create a Frequency
            Table</b></p>
    <p class="mb-3 leading-relaxed text-align=justify" style="margin-left: 20px;">
        Next, we will create a frequency table of all the stemmed words in the article content. We will use the NLTK
        library to tokenize the text into words and stem each word using the PorterStemmer algorithm. We will also
        remove any stop words (common words like "the" or "a" that don't carry significant meaning) from the text.<br>
        <pre><code class="python">
            from nltk.corpus import stopwords
            from nltk.stem import PorterStemmer
            from nltk.tokenize import word_tokenize
            
            def create_frequency_table(text_string):
                stop_words = set(stopwords.words("english"))
                words = word_tokenize(text_string)
                stemmer = PorterStemmer()
            
                freq_table = dict()
                for word in words:
                    word = stemmer.stem(word)
                    if word in stop_words:
                        continue
                    if word in freq_table:
                        freq_table[word] += 1
                    else:
                        freq_table[word] = 1
            
                return freq_table
            
      </code></pre>
    </p>

    <p class="mb-8 leading-relaxed text-align=justify" style="margin-left: 20px;"><b>Step 3: Score Sentences</b></p>
    <p class="mb-2 leading-relaxed text-align=justify" style="margin-left: 20px;">After creating the frequency table, we
        will
        score each sentence in the article based on the frequency of its non-stop words. We will iterate through each
        sentence, count the number of words in the sentence that are not stop words, and calculate the sum of the
        frequencies of these words. We will store this score in a dictionary with the first 10 characters of the
        sentence as the key and the score as the value.<br></p>
    <pre><code class="python">
        from nltk.tokenize import sent_tokenize

        def score_sentences(sentences, freq_table):
            sentence_value = dict()
            for sentence in sentences:
                word_count_in_sentence = len(word_tokenize(sentence))
                word_count_in_sentence_except_stop_words = 0
                for word_value in freq_table:
                    if word_value in sentence.lower():
                        word_count_in_sentence_except_stop_words += 1
                        if sentence[:10] in sentence_value:
                            sentence_value[sentence[:10]] += freq_table[word_value]
                        else:
                            sentence_value[sentence[:10]] = freq_table[word_value]
        
                if sentence[:10] in sentence_value:
                    sentence_value[sentence[:10]] = sentence_value[sentence[:10]] / word_count_in_sentence_except_stop_words
        
            return sentence_value        

      </code></pre>
    <p class="mb-8 leading-relaxed text-align=justify" style="margin-left: 20px;"><b> Step 4: Find the Average Score
        </b></p>
    <p class="mb-2 leading-relaxed text-align=justify" style="margin-left: 20px;">To find the average score, we
        calculate the sum
        of all the scores and divide it by the total number of scores. In our case, the scores are the values in the
        sentence_value dictionary. We can use the Python sum() function to calculate the sum of the values, and then
        divide it by the length of the dictionary to get the average score.<br></p>
    <pre><code class="python">
        def find_average_score(sentence_value):
            """
            Calculates the average score of the sentences.
            """
            sum_values = sum(sentence_value.values())
            average = sum_values / len(sentence_value)
            return average
        </code></pre>


    <p class="mb-8 leading-relaxed text-align=justify" style="margin-left: 20px;"><b> Step 5: Generate the Summary </b>
    </p>
    <p class="mb-2 leading-relaxed text-align=justify" style="margin-left: 20px;">To generate the summary, we iterate
        over each
        sentence in the sentences list and check if its score is greater than or equal to the threshold value. If it is,
        we add the sentence to the summary string.<br></p>
    <pre><code class="python">
        def generate_summary(sentences, sentence_value, threshold):
            """
            Generates the summary of the article using the sentence scores and threshold.
            """
            summary = ''
            for sentence in sentences:
                if sentence[:10] in sentence_value and sentence_value[sentence[:10]] >= threshold:
                    summary += " " + sentence
            return summary

        </code></pre>

    <p class="mb-8 leading-relaxed text-align=justify" style="margin-left: 20px;"><b> Step 6: Run the Summarization
            Algorithm
        </b></p>
    <p class="mb-2 leading-relaxed text-align=justify" style="margin-left: 20px;">Then we put everything together and
        run
        the summarization algorithm. In the run_summarization function, we first create the frequency table using the
        create_frequency_table function, then tokenize the sentences using the sent_tokenize function from the nltk
        library. We then call the score_sentences function to get the sentence scores, and use the find_average_score
        function to find the threshold. Finally, we call the generate_summary function to generate the summary.<br></p>
    <pre><code class="python">
        def run_summarization(article_content):
            """
            Runs the summarization algorithm on the input article content.
            """
            # Create frequency table
            freq_table = create_frequency_table(article_content)

            # Tokenize sentences
            sentences = sent_tokenize(article_content)

            # Score the sentences
            sentence_value = score_sentences(sentences, freq_table)

            # Find the threshold
            threshold = 1.3 * find_average_score(sentence_value)

            # Generate the summary
            summary = generate_summary(sentences, sentence_value, threshold)

            return summary

        </code></pre>

    <p class="mb-8 leading-relaxed text-align=justify" style="margin-left: 20px;"><b> Step 7:
        </b></p>
    <p class="mb-2 leading-relaxed text-align=justify" style="margin-left: 20px;">Finally, we find the wikipedia article
        that we
        want to summarize and execute the main function that runs the entire summarization process.
        By running this script, you will get the summary of the Wikipedia article on Machine Learning. You can modify
        the URL to summarize other articles as well.<br></p>
    <pre><code class="python">
        if __name__ == '__main__':
            # Define the web page URL and fetch the content
            url = 'https://en.wikipedia.org/wiki/Machine_Learning'
            article_content = fetch_article_content(url)
            clean_string = url.split('/')[-1].replace('_', ' ')
            # Generate the summary
            summary = clean_string+": "+ run_summarization(article_content)
            # Print the summary
            print(summary)

        </code></pre>


    <p class="mb-8 leading-relaxed text-align=justify" style="margin-left: 20px;"><b>stopwords</b>: This resource
        contains a list
        of common stop words (words that are frequently occurring and do not carry any meaning, e.g. "the", "and", "is",
        "a") that are used to remove noise from text data during text preprocessing.<br>

        <section>
            <p class="mb-8 leading-relaxed text-align=justify" style="margin-left: 20px; font-size: 20px;">
                <b>CONCLUSION</b></p>

            <p class="mb-8 leading-relaxed text-align=justify" style="margin-left: 20px;">In this tutorial, we learned
                how to
                summarize text using a simple and effective algorithm. We first fetched the content of a webpage and
                created a frequency table of its words. Then, we scored each sentence based on the frequency of its
                non-stop words, found the average score, and generated a summary using a threshold value. Finally, we
                applied this algorithm to summarize an article on machine learning from Wikipedia. This tutorial
                demonstrates how we can use basic natural language processing techniques to create a useful
                summarization tool. However, it is worth noting that more advanced techniques, such as deep
                learning-based methods, may be necessary for handling larger volumes of text and generating more
                accurate summaries. <br>
                <br>
                If you found the tutorial interesting, here is a bonus for you.<br>

                <p class="mb-8 leading-relaxed text-align=justify" style="margin-left: 20px;"><b> Using BART Transformer
                        for text
                        summarization
                    </b></p>
                <p class="mb-2 leading-relaxed text-align=justify" style="margin-left: 20px;">This code generates a
                    summary of a
                    given
                    input text using BART (Bidirectional and Auto-Regressive Transformer) transformer. BART is a
                    state-of-the-art pre-trained language model developed by Facebook AI that can perform a wide range
                    of natural language processing tasks. The code uses the BART model to generate a summary by first
                    tokenizing the input text and then generating a summary using beam search. The resulting summary is
                    decoded from the tokenized summary IDs and returned as a string.<br></p>
                <pre><code class="python">
                    from transformers import BartTokenizer, TFBartForConditionalGeneration

                    def BART_generate_summary(text: str, model_name: str = "facebook/bart-large-cnn") -> str:
                        """
                        Generate summary using BART transformer
                        """
                        # Initialize tokenizer and model
                        tokenizer = BartTokenizer.from_pretrained(model_name, from_tf=True)
                        model = TFBartForConditionalGeneration.from_pretrained(model_name)

                        # Tokenize input text
                        inputs = tokenizer.batch_encode_plus([text], max_length=1024, return_tensors='tf', truncation=True)

                        # Generate summary
                        summary_ids = model.generate(inputs['input_ids'], num_beams=4, max_length=200, early_stopping=True)
                        summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)

                        return summary

                    bart_summary = BART_generate_summary(article_content)
                    print(bart_summary)
            
                </code></pre>


            </p>
        </section>
        <br>
        <section>
            <p class="mb-8 leading-relaxed text-align=justify" style="margin-left: 20px; font-size: 20px;"><b>What makes
                    this
                    tutorial different?</b>
            </p>

            <p class="mb-8 leading-relaxed text-align=justify" style="margin-left: 20px; font-size: 20px;"> This
                tutorial is
                designed to teach how
                to extract the most important information from a
                long piece of text and present it in a shorter format. The tutorial is useful in situations where there
                is a large amount of text that needs to be quickly processed and understood.

                Some common uses of text summarization tutorials and tools include:
            </p>
            <ul class="list-disc mb-8 leading-relaxed text-align=justify" style="margin-left: 60px; font-size: 20px;">
                <li>Summarizing news articles and research papers</li>
                <li>Creating summaries of legal documents</li>
                <li>Generating briefs for legal cases</li>
                <li>Summarizing social media posts</li>
                <li>Creating product descriptions</li>
            </ul>
            <p class="mb-8 leading-relaxed text-align=justify" style="margin-left: 20px; font-size: 20px;">
                The tutorial is designed to teach how to implement a text summarization tool for the English
                language. However, the approach can be adapted to work with other languages by modifying the stop word
                list and stemmer accordingly.

                There are similar tutorials and tools available for text summarization, but each tutorial is unique in
                its implementation and may focus on different aspects of the summarization process.Majority of the
                tutorials available use the textRank algorithm for summarization, which may require more computational
                resources than the other code, particularly for longer texts. On the other hand, this utorial uses a
                word
                frequency-based approach to summarization. This method is generally faster than the TextRank algorithm,
                and can work well when a quick summary is needed.

                One unique feature of the tutorial in is its use of frequency tables to score sentences based on
                the frequency of their non-stop words. This approach allows the tool to quickly identify the most
                important sentences in a piece of text, making it a useful tool for summarizing large volumes of
                information.</p>
            <br>
    </p>
</section>



<section>
    <p class="mb-8 leading-relaxed text-align=justify" style="margin-left: 20px; font-size: 15px;"><b>References:</b>
    </p>

    <p class="mb-8 leading-relaxed text-align=justify" style="margin-left: 20px;">
        <a href="https://www.nltk.org/" style="color:blue; text-decoration:underline;">Natural Language Toolkit</a><br>
        <a href="https://en.wikipedia.org/wiki/Automatic_summarization"
            style="color:blue; text-decoration:underline;">Automatic Summarization</a><br>
        <a href="https://huggingface.co/facebook/bart-large-cnn" style="color:blue; text-decoration:underline;">BART
            large-sized model(fine-tuned on CNN Daily Mail)
        </a><br>
    </p>
</section>

</html>
